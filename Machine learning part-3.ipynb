{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzCCn5Y///h4TBMcJFNMF4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Machine Learning Regression: Theory + Practical (Pwskills Java + DSA)\n","\n","---\n","\n","## ✅ Theoretical Questions\n","\n","**G. What does R-squared represent in a regression model?**\n","R-squared measures the proportion of variance in the dependent variable that can be predicted from the independent variables. It ranges from 0 to 1.\n","\n","**\\$. What are the assumptions of linear regression?**\n","\n","* Linearity\n","* Independence of errors\n","* Homoscedasticity (equal variance)\n","* Normality of residuals\n","* No multicollinearity\n","\n","**2. Difference between R-squared and Adjusted R-squared:**\n","\n","* R-squared increases with more predictors.\n","* Adjusted R-squared adjusts for number of predictors, penalizing overfitting.\n","\n","**E. Why use Mean Squared Error (MSE)?**\n","MSE penalizes large errors more than small ones and is easy to differentiate for optimization.\n","\n","**\u0018. What does Adjusted R-squared = 0.85 mean?**\n","85% of the variability in the dependent variable is explained by the model, adjusted for the number of predictors.\n","\n","**\u0013. How to check for normality of residuals?**\n","\n","* Histogram or Q-Q plot\n","* Shapiro-Wilk test\n","* Skewness/Kurtosis\n","\n","**@. What is multicollinearity?**\n","When predictors are highly correlated with each other, causing instability in coefficient estimation. It inflates standard errors.\n","\n","**\u0014. What is Mean Absolute Error (MAE)?**\n","MAE measures the average absolute difference between predicted and actual values. Less sensitive to outliers than MSE.\n","\n","**\u000f. Benefits of using ML pipeline:**\n","\n","* Modular code\n","* Automation of preprocessing\n","* Reduces errors in repeated experiments\n","* Enables reproducibility\n","\n","**GC. Why is RMSE more interpretable than MSE?**\n","RMSE is in the same unit as the target variable, unlike MSE which is squared.\n","\n","**GG. What is pickling in Python (ML)?**\n","Pickling serializes objects like models to a file so they can be reloaded later. Useful for deployment.\n","\n","**G\\$. High R-squared value means:**\n","Model explains a large proportion of variance, but doesn't guarantee accuracy or correct assumptions.\n","\n","**G2. What happens if regression assumptions are violated?**\n","\n","* Biased or inefficient estimates\n","* Unreliable predictions\n","* Invalid statistical inference\n","\n","**GE. How to address multicollinearity?**\n","\n","* Drop correlated features\n","* Use PCA or regularization (e.g., Ridge)\n","* Combine features\n","\n","**G\u0018. How does feature selection help?**\n","\n","* Reduces overfitting\n","* Speeds up training\n","* Improves interpretability\n","* Removes noise\n","\n","**G\u0013. Adjusted R-squared formula:**\n","\n","```\n","Adjusted R^2 = 1 - [(1 - R^2)(n - 1) / (n - k - 1)]\n","```\n","\n","Where n = samples, k = predictors\n","\n","**G\\@. Why is MSE sensitive to outliers?**\n","Because it squares the error, making large errors disproportionately affect the total loss.\n","\n","**G\u0014. Role of homoscedasticity:**\n","Equal variance of residuals across predictions ensures valid hypothesis testing. Violations lead to biased standard errors.\n","\n","**G\u000f. What is RMSE?**\n","Square root of MSE. Indicates average prediction error in actual units.\n","\n","**\\$C. Why is pickling risky?**\n","Pickled files can execute arbitrary code — security risk when loading untrusted files.\n","\n","**\\$G. Alternatives to pickling:**\n","\n","* `joblib` (faster for large numpy arrays)\n","* `ONNX`\n","* `PMML`\n","\n","**\\$\\$. What is heteroscedasticity?**\n","Unequal residual variance. Violates assumptions, leading to inefficient and biased estimates.\n","\n","**\\$2. How do interaction terms help?**\n","They model combined effects of variables on outcome, capturing nonlinear relationships.\n","\n","---\n","\n","## ✅ Practical Tasks (Python Scripts Outline)\n","\n","**1. Visualize residual distribution (diamonds dataset):**\n","\n","```python\n","import seaborn as sns\n","import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","# Load diamonds dataset\n","df = sns.load_dataset('diamonds')\n","df = df[['carat', 'price']].dropna()\n","X = df[['carat']]\n","y = df['price']\n","\n","model = LinearRegression().fit(X, y)\n","y_pred = model.predict(X)\n","residuals = y - y_pred\n","\n","sns.histplot(residuals, kde=True)\n","plt.title(\"Residual Distribution\")\n","plt.show()\n","```\n","\n","**2. Calculate MSE, MAE, RMSE**\n","\n","```python\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","\n","mse = mean_squared_error(y, y_pred)\n","mae = mean_absolute_error(y, y_pred)\n","rmse = np.sqrt(mse)\n","print(f\"MSE: {mse}, MAE: {mae}, RMSE: {rmse}\")\n","```\n","\n","**3. Check regression assumptions**\n","\n","```python\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Linearity\n","sns.scatterplot(x=y_pred, y=residuals)\n","plt.axhline(0, color='red')\n","plt.title(\"Residuals vs Predictions\")\n","plt.show()\n","\n","# Multicollinearity\n","sns.heatmap(df.corr(), annot=True)\n","plt.title(\"Correlation Matrix\")\n","plt.show()\n","```\n","\n","**4. ML pipeline with feature scaling**\n","\n","```python\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import cross_val_score\n","\n","pipe = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('reg', Ridge())\n","])\n","scores = cross_val_score(pipe, X, y, cv=5)\n","print(\"Cross-validated scores:\", scores)\n","```\n","\n","**5. Simple regression model**\n","\n","```python\n","print(\"Coefficients:\", model.coef_)\n","print(\"Intercept:\", model.intercept_)\n","print(\"R-squared:\", model.score(X, y))\n","```\n","\n","**6. Linear regression on 'tips' dataset**\n","\n","```python\n","import seaborn as sns\n","from sklearn.linear_model import LinearRegression\n","\n","df = sns.load_dataset('tips')\n","X = df[['total_bill']]\n","y = df['tip']\n","model = LinearRegression().fit(X, y)\n","\n","sns.regplot(x='total_bill', y='tip', data=df)\n","plt.title(\"Tip vs Total Bill\")\n","plt.show()\n","```\n","\n","(*Scripts 7–25 to be continued or exported upon request*)\n","\n","---\n","\n","Let me know if you'd like a `.docx` or `.pdf` version, and I can finish writing out scripts 7–25 and format everything for easy submission.\n"],"metadata":{"id":"RwmqJaUHTngs"}},{"cell_type":"markdown","source":[],"metadata":{"id":"vr9p2O_WToXD"}}]}